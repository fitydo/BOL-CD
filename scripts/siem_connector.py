#!/usr/bin/env python3
"""
SIEM Connector - Real-time event streaming from Splunk/Sentinel/OpenSearch
"""
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
import argparse
from typing import Dict, List, Any

class SIEMConnector:
    def __init__(self, source: str, config: Dict[str, Any]):
        self.source = source
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'data/raw'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def connect_splunk(self) -> List[Dict]:
        """Splunk REST API connection"""
        import os
        import requests
        # from urllib.parse import quote  # unused
        
        # Áí∞Â¢ÉÂ§âÊï∞„Åã„ÇâË™çË®ºÊÉÖÂ†±„ÇíÂèñÂæó
        splunk_url = os.getenv('BOLCD_SPLUNK_URL')
        splunk_token = os.getenv('BOLCD_SPLUNK_TOKEN')
        auth_scheme = os.getenv('BOLCD_SPLUNK_AUTH_SCHEME', 'Splunk')
        
        if splunk_url and splunk_token:
            try:
                # Splunk REST API „Çí‰ΩøÁî®„Åó„Å¶„É™„Ç¢„É´„Éá„Éº„Çø„ÇíÂèñÂæó
                headers = {
                    'Authorization': f'{auth_scheme} {splunk_token}',
                    'Content-Type': 'application/json'
                }
                
                # „Çª„Ç≠„É•„É™„ÉÜ„Ç£„Ç§„Éô„É≥„Éà„ÇíÊ§úÁ¥¢
                search_query = 'search index=_internal OR index=main earliest=-1h | head 500'
                
                # „Ç∏„Éß„Éñ„Çí‰ΩúÊàê
                job_endpoint = f"{splunk_url}/services/search/jobs"
                job_data = {
                    'search': search_query,
                    'output_mode': 'json'
                }
                
                response = requests.post(job_endpoint, headers=headers, data=job_data, verify=False)
                
                if response.status_code == 201:
                    # „Ç∏„Éß„ÉñID„ÇíÂèñÂæó
                    job_id = response.json().get('sid')
                    
                    # ÁµêÊûú„ÇíÂèñÂæó
                    results_endpoint = f"{splunk_url}/services/search/jobs/{job_id}/results"
                    results = requests.get(results_endpoint, headers=headers, params={'output_mode': 'json'}, verify=False)
                    
                    if results.status_code == 200:
                        events = []
                        for result in results.json().get('results', []):
                            events.append({
                                "_time": result.get('_time', datetime.now().isoformat()),
                                "host": result.get('host', 'unknown'),
                                "source": result.get('source', 'unknown'),
                                "sourcetype": result.get('sourcetype', 'unknown'),
                                "signature": result.get('signature', 'Unknown_Event'),
                                "severity": result.get('severity', 'medium'),
                                "rule_id": f"RULE-{hash(result.get('_raw', '')) % 1000:04d}",
                                "entity_id": result.get('host', 'unknown'),
                                "count": 1,
                                "_raw": result.get('_raw', '')
                            })
                        
                        if events:
                            print(f"‚úÖ Splunk API„Åã„Çâ{len(events)}‰ª∂„ÅÆ„É™„Ç¢„É´„Ç§„Éô„É≥„Éà„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü")
                            return events
                        
            except Exception as e:
                print(f"‚ö†Ô∏è Splunk APIÊé•Á∂ö„Ç®„É©„Éº: {e}")
                print("„Éá„É¢„Éá„Éº„Çø„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„Åó„Åæ„Åô")
        
        # „Éá„É¢„Éá„Éº„Çø„Å´„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        print("üì¶ „Éá„É¢„Éá„Éº„Çø„ÇíÁîüÊàê‰∏≠...")
        # Demo: Generate realistic Splunk-like events
        events = []
        now = datetime.now()
        
        # Simulate real Splunk search results
        alert_patterns = [
            {"signature": "Failed_SSH_Login", "severity": "medium", "source": "auth.log"},
            {"signature": "SQL_Injection_Attempt", "severity": "high", "source": "waf"},
            {"signature": "Port_Scan_Detected", "severity": "low", "source": "firewall"},
            {"signature": "Privilege_Escalation", "severity": "critical", "source": "endpoint"},
            {"signature": "Data_Exfiltration", "severity": "high", "source": "dlp"},
            {"signature": "Malware_Detected", "severity": "critical", "source": "antivirus"},
            {"signature": "Unauthorized_Access", "severity": "medium", "source": "access_log"},
            {"signature": "DDoS_Attack", "severity": "high", "source": "network"},
        ]
        
        hosts = ["web-prod-01", "web-prod-02", "db-master-01", "db-slave-01", 
                 "app-server-01", "app-server-02", "proxy-01", "proxy-02",
                 "mail-server-01", "file-server-01"]
        
        # Generate events for the last hour
        num_events = self.config.get('num_events', 500)  # Default to 500 events
        for i in range(num_events):
            pattern = alert_patterns[i % len(alert_patterns)]
            events.append({
                "_time": (now - timedelta(minutes=i % 60)).isoformat(),
                "host": hosts[i % len(hosts)],
                "source": pattern["source"],
                "sourcetype": "security:alert",
                "signature": pattern["signature"],
                "severity": pattern["severity"],
                "rule_id": f"RULE-{hash(pattern['signature']) % 1000:04d}",
                "entity_id": hosts[i % len(hosts)],
                "count": 1 + (i % 5),
                "_raw": f"Alert: {pattern['signature']} on {hosts[i % len(hosts)]}"
            })
        
        return events
    
    def connect_sentinel(self) -> List[Dict]:
        """Azure Sentinel connection via API"""
        # In production, use Azure SDK
        # from azure.monitor.query import LogsQueryClient
        # from azure.identity import DefaultAzureCredential
        
        # Demo: Generate Sentinel-like events
        events = []
        now = datetime.now()
        
        for i in range(50):
            events.append({
                "TimeGenerated": (now - timedelta(minutes=i*2)).isoformat(),
                "Computer": f"AZ-VM-{i % 5:02d}",
                "AlertName": f"SecurityAlert_{i % 8}",
                "AlertSeverity": ["Low", "Medium", "High", "Critical"][i % 4],
                "Category": ["NetworkSecurity", "IdentityProtection", "DataProtection"][i % 3],
                "entity_id": f"AZ-VM-{i % 5:02d}",
                "rule_id": f"SENT-{i % 10:03d}"
            })
        
        return events
    
    def connect_opensearch(self) -> List[Dict]:
        """OpenSearch connection"""
        # In production, use opensearch-py
        # from opensearchpy import OpenSearch
        
        # Demo: Generate OpenSearch-like events
        events = []
        now = datetime.now()
        
        for i in range(75):
            events.append({
                "@timestamp": (now - timedelta(minutes=i)).isoformat(),
                "host.name": f"elastic-node-{i % 6}",
                "event.module": "security",
                "event.dataset": "alert",
                "event.severity": 1 + (i % 5),
                "rule.name": f"Detection_Rule_{i % 12}",
                "entity_id": f"elastic-node-{i % 6}",
                "rule_id": f"ELASTIC-{i % 15:03d}"
            })
        
        return events
    
    def fetch_events(self, hours_back: int = 1) -> List[Dict]:
        """Fetch events from configured SIEM"""
        if self.source == 'splunk':
            return self.connect_splunk()
        elif self.source == 'sentinel':
            return self.connect_sentinel()
        elif self.source == 'opensearch':
            return self.connect_opensearch()
        else:
            raise ValueError(f"Unsupported SIEM source: {self.source}")
    
    def save_events(self, events: List[Dict], filename: str = None) -> str:
        """Save events to JSONL file"""
        if not filename:
            filename = f"events_{self.source}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.jsonl"
        
        filepath = self.output_dir / filename
        with open(filepath, 'w') as f:
            for event in events:
                # Normalize fields for AB testing
                normalized = {
                    'timestamp': event.get('_time') or event.get('TimeGenerated') or event.get('@timestamp'),
                    'entity_id': event.get('entity_id') or event.get('host') or event.get('Computer') or event.get('host.name'),
                    'rule_id': event.get('rule_id') or event.get('AlertName') or event.get('rule.name'),
                    'signature': event.get('signature') or event.get('AlertName') or event.get('rule.name'),
                    'severity': event.get('severity') or event.get('AlertSeverity') or event.get('event.severity'),
                    'source': event.get('source') or event.get('Category') or event.get('event.module'),
                    'raw': json.dumps(event)
                }
                f.write(json.dumps(normalized) + '\n')
        
        return str(filepath)
    
    def stream_events(self, interval_seconds: int = 60):
        """Continuously stream events"""
        print(f"üîÑ Starting real-time streaming from {self.source} (interval: {interval_seconds}s)")
        
        while True:
            try:
                events = self.fetch_events(hours_back=1)
                if events:
                    filepath = self.save_events(events)
                    print(f"‚úÖ Saved {len(events)} events to {filepath}")
                    
                    # Trigger A/B processing
                    self.trigger_ab_processing(filepath)
                else:
                    print(f"‚ö†Ô∏è No new events from {self.source}")
                
                time.sleep(interval_seconds)
                
            except KeyboardInterrupt:
                print("\nüõë Streaming stopped")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
                time.sleep(interval_seconds)
    
    def trigger_ab_processing(self, filepath: str):
        """Trigger A/B split and report generation"""
        import subprocess
        
        try:
            # Run A/B split
            result = subprocess.run(
                f"python scripts/ab/ab_split.py --in {filepath} --out-dir data/ab --key-fields entity_id,rule_id",
                shell=True, capture_output=True, text=True
            )
            
            if result.returncode == 0:
                # Run A/B report
                date_label = datetime.now().strftime('%Y-%m-%d')
                subprocess.run(
                    f"python scripts/ab/ab_report.py --in-a data/ab/A.jsonl --in-b data/ab/B.jsonl --out-dir reports --date-label {date_label}",
                    shell=True
                )
                print(f"üìä A/B report updated for {date_label}")
        except Exception as e:
            print(f"‚ö†Ô∏è A/B processing error: {e}")


def main():
    parser = argparse.ArgumentParser(description='SIEM Connector for BOL-CD')
    parser.add_argument('--source', choices=['splunk', 'sentinel', 'opensearch'], 
                        default='splunk', help='SIEM source')
    parser.add_argument('--mode', choices=['fetch', 'stream'], 
                        default='fetch', help='Operation mode')
    parser.add_argument('--interval', type=int, default=60, 
                        help='Streaming interval in seconds')
    parser.add_argument('--output-dir', default='data/raw', 
                        help='Output directory for events')
    parser.add_argument('--config', help='Config file path (JSON)')
    
    args = parser.parse_args()
    
    # Load config
    config = {'output_dir': args.output_dir}
    if args.config and Path(args.config).exists():
        with open(args.config) as f:
            config.update(json.load(f))
    
    # Initialize connector
    connector = SIEMConnector(args.source, config)
    
    if args.mode == 'fetch':
        # One-time fetch
        events = connector.fetch_events()
        filepath = connector.save_events(events)
        print(f"‚úÖ Fetched {len(events)} events from {args.source}")
        print(f"üìÅ Saved to: {filepath}")
    else:
        # Continuous streaming
        connector.stream_events(interval_seconds=args.interval)


if __name__ == '__main__':
    main()
