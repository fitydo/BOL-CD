#!/usr/bin/env python3
"""
A/B ML Pipeline - æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–
"""
import json
import subprocess
import argparse
from pathlib import Path
from typing import Dict, Tuple
from datetime import datetime
import numpy as np

class ABMLPipeline:
    def __init__(self, config_path: str = "config/production_rules.json"):
        self.config_path = Path(config_path)
        self.metrics_history = []
        self.models = {}
        
    def run_command(self, cmd: str) -> Tuple[int, str, str]:
        """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.returncode, result.stdout, result.stderr
    
    def collect_data(self, source: str = "splunk", hours: int = 24) -> Path:
        """ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚º"""
        print(f"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿åé›†ä¸­ (source={source}, hours={hours})...")
        
        # SIEMã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        cmd = f"python scripts/siem_connector.py --source {source} --mode fetch"
        returncode, stdout, stderr = self.run_command(cmd)
        
        if returncode != 0:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {stderr}")
            return None
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        latest_files = sorted(Path("data/raw").glob("events_*.jsonl"), 
                            key=lambda p: p.stat().st_mtime, reverse=True)
        
        if latest_files:
            return latest_files[0]
        return None
    
    def feature_engineering(self, input_file: Path) -> Dict:
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­...")
        
        events = []
        with open(input_file, 'r') as f:
            for line in f:
                if line.strip():
                    events.append(json.loads(line))
        
        features = {
            'total_events': len(events),
            'unique_entities': len(set(e.get('entity_id', '') for e in events)),
            'unique_rules': len(set(e.get('rule_id', '') for e in events)),
            'severity_distribution': {},
            'time_distribution': {},
            'top_patterns': []
        }
        
        # é‡è¦åº¦åˆ†å¸ƒ
        severity_counts = {}
        for e in events:
            sev = e.get('severity', 'unknown')
            severity_counts[sev] = severity_counts.get(sev, 0) + 1
        features['severity_distribution'] = severity_counts
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦TOP10
        pattern_counts = {}
        for e in events:
            pattern = f"{e.get('entity_id')}:{e.get('rule_id')}"
            pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        top_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        features['top_patterns'] = [{'pattern': p, 'count': c} for p, c in top_patterns]
        
        return features
    
    def train_model(self, input_file: Path) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º"""
        print("ğŸ§  ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
        
        # Robust Optimizerã§å­¦ç¿’
        cmd = f"python scripts/ab_robust_optimizer.py --input {input_file} --target-reduction 0.5"
        returncode, stdout, stderr = self.run_command(cmd)
        
        if returncode != 0:
            print(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {stderr}")
            return None
        
        # å­¦ç¿’çµæœã‚’è§£æ
        lines = stdout.split('\n')
        metrics = {}
        for line in lines:
            if 'å‰Šæ¸›ç‡:' in line and 'æœ€çµ‚ãƒ¢ãƒ‡ãƒ«' in line:
                # å‰Šæ¸›ç‡ã‚’æŠ½å‡º
                parts = line.split(':')
                if len(parts) >= 2:
                    rate_str = parts[-1].strip().replace('%', '')
                    try:
                        metrics['reduction_rate'] = float(rate_str) / 100
                    except Exception:
                        pass
            elif 'ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆæ„åº¦:' in line:
                parts = line.split(':')
                if len(parts) >= 2:
                    try:
                        metrics['ensemble_agreement'] = float(parts[-1].strip())
                    except Exception:
                        pass
        
        return metrics
    
    def apply_rules(self, input_file: Path, rules_file: Path) -> Tuple[Path, Dict]:
        """ãƒ«ãƒ¼ãƒ«é©ç”¨ãƒ•ã‚§ãƒ¼ã‚º"""
        print("ğŸ“‹ ãƒ«ãƒ¼ãƒ«é©ç”¨ä¸­...")
        
        # ãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(rules_file, 'r') as f:
            rules = json.load(f)
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
        events = []
        with open(input_file, 'r') as f:
            for line in f:
                if line.strip():
                    events.append(json.loads(line))
        
        # ãƒ«ãƒ¼ãƒ«é©ç”¨ãƒ­ã‚¸ãƒƒã‚¯
        suppressed = []
        passed = []
        
        for event in events:
            should_suppress = False
            
            # æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            for rule in rules.get('suppression_rules', []):
                pattern = f"{event.get('entity_id')}:{event.get('rule_id')}"
                if pattern == rule['pattern']:
                    should_suppress = True
                    break
            
            if should_suppress:
                suppressed.append(event)
            else:
                passed.append(event)
        
        # çµæœã‚’ä¿å­˜
        output_file = Path(f"data/ab/pipeline_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl")
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            for event in passed:
                f.write(json.dumps(event, ensure_ascii=False) + '\n')
        
        metrics = {
            'total_events': len(events),
            'suppressed_count': len(suppressed),
            'passed_count': len(passed),
            'reduction_rate': len(suppressed) / len(events) if events else 0
        }
        
        return output_file, metrics
    
    def evaluate(self, metrics: Dict) -> Dict:
        """è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º"""
        print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ä¸­...")
        
        evaluation = {
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics,
            'performance': {}
        }
        
        # ç›®æ¨™ã¨ã®æ¯”è¼ƒ
        target_reduction = 0.5
        actual_reduction = metrics.get('reduction_rate', 0)
        
        evaluation['performance']['target_achievement'] = actual_reduction / target_reduction
        evaluation['performance']['is_acceptable'] = actual_reduction >= target_reduction * 0.9
        
        # å±¥æ­´ã«è¿½åŠ 
        self.metrics_history.append(evaluation)
        
        # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ï¼ˆç›´è¿‘5å›ï¼‰
        if len(self.metrics_history) >= 2:
            recent_rates = [h['metrics'].get('reduction_rate', 0) 
                          for h in self.metrics_history[-5:]]
            evaluation['performance']['moving_average'] = np.mean(recent_rates)
            evaluation['performance']['trend'] = 'improving' if recent_rates[-1] > recent_rates[0] else 'declining'
        
        return evaluation
    
    def run_pipeline(self, source: str = "splunk") -> Dict:
        """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("\n" + "="*60)
        print("ğŸš€ A/Bæœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        print("="*60)
        
        results = {}
        
        # 1. ãƒ‡ãƒ¼ã‚¿åé›†
        data_file = self.collect_data(source)
        if not data_file:
            return {'error': 'Data collection failed'}
        results['data_file'] = str(data_file)
        
        # 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        features = self.feature_engineering(data_file)
        results['features'] = features
        
        # 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        train_metrics = self.train_model(data_file)
        if train_metrics:
            results['training'] = train_metrics
        
        # 4. ãƒ«ãƒ¼ãƒ«é©ç”¨
        output_file, apply_metrics = self.apply_rules(data_file, self.config_path)
        results['application'] = apply_metrics
        results['output_file'] = str(output_file)
        
        # 5. è©•ä¾¡
        evaluation = self.evaluate(apply_metrics)
        results['evaluation'] = evaluation
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ“ˆ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœ")
        print("="*60)
        print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {features['total_events']}ä»¶")
        print(f"å‰Šæ¸›ç‡: {apply_metrics['reduction_rate']*100:.1f}%")
        print(f"ç›®æ¨™é”æˆåº¦: {evaluation['performance']['target_achievement']*100:.1f}%")
        
        if evaluation['performance']['is_acceptable']:
            print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™")
        else:
            print("âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãŒå¿…è¦ã§ã™")
        
        if 'moving_average' in evaluation['performance']:
            print(f"ç§»å‹•å¹³å‡: {evaluation['performance']['moving_average']*100:.1f}%")
            print(f"ãƒˆãƒ¬ãƒ³ãƒ‰: {evaluation['performance']['trend']}")
        
        return results
    
    def continuous_learning(self, interval_minutes: int = 60):
        """ç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰"""
        print(f"â™¾ï¸ ç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ (é–“éš”: {interval_minutes}åˆ†)")
        
        while True:
            try:
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
                results = self.run_pipeline()
                
                # çµæœã‚’ä¿å­˜
                report_file = Path(f"reports/pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                report_file.parent.mkdir(parents=True, exist_ok=True)
                with open(report_file, 'w') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                
                print(f"\nğŸ’¾ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
                
                # å¾…æ©Ÿ
                print(f"\nâ° æ¬¡å›å®Ÿè¡Œã¾ã§{interval_minutes}åˆ†å¾…æ©Ÿ...")
                import time
                time.sleep(interval_minutes * 60)
                
            except KeyboardInterrupt:
                print("\nğŸ›‘ ç¶™ç¶šå­¦ç¿’ã‚’åœæ­¢ã—ã¾ã—ãŸ")
                break
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"â° {interval_minutes}åˆ†å¾Œã«å†è©¦è¡Œ...")
                import time
                time.sleep(interval_minutes * 60)


def main():
    parser = argparse.ArgumentParser(description='A/B ML Pipeline')
    parser.add_argument('--mode', choices=['single', 'continuous'], default='single',
                       help='Execution mode')
    parser.add_argument('--source', default='splunk',
                       help='Data source')
    parser.add_argument('--interval', type=int, default=60,
                       help='Interval for continuous mode (minutes)')
    
    args = parser.parse_args()
    
    pipeline = ABMLPipeline()
    
    if args.mode == 'single':
        # å˜ç™ºå®Ÿè¡Œ
        results = pipeline.run_pipeline(args.source)
        
        # çµæœã‚’ä¿å­˜
        output_file = Path(f"reports/ml_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ å®Œå…¨ãªãƒ¬ãƒãƒ¼ãƒˆ: {output_file}")
        
    else:
        # ç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
        pipeline.continuous_learning(args.interval)
    
    return 0


if __name__ == '__main__':
    exit(main())
