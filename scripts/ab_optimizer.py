#!/usr/bin/env python3
"""
A/B Test Optimizer - æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å‰Šæ¸›ç‡æœ€é©åŒ–
"""
import json
import argparse
from typing import Dict, List, Tuple
from collections import Counter, defaultdict
import numpy as np
from datetime import datetime

class ABOptimizer:
    def __init__(self):
        self.feature_weights = {
            'entity_frequency': 0.3,
            'rule_frequency': 0.25,
            'time_clustering': 0.2,
            'signature_similarity': 0.15,
            'severity_weight': 0.1
        }
        self.learned_patterns = {}
        self.suppression_scores = {}
        
    def extract_features(self, events: List[Dict]) -> Dict:
        """ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡º"""
        features = {
            'entity_counts': Counter(),
            'rule_counts': Counter(),
            'signature_counts': Counter(),
            'time_clusters': defaultdict(list),
            'severity_distribution': Counter(),
            'pattern_sequences': []
        }
        
        for event in events:
            entity = event.get('entity_id', 'unknown')
            rule = event.get('rule_id', 'unknown')
            signature = event.get('signature', 'unknown')
            severity = event.get('severity', 'low')
            
            features['entity_counts'][entity] += 1
            features['rule_counts'][rule] += 1
            features['signature_counts'][signature] += 1
            features['severity_distribution'][severity] += 1
            
            # æ™‚é–“ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆ1æ™‚é–“å˜ä½ï¼‰
            if 'timestamp' in event:
                try:
                    ts = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
                    hour_key = ts.strftime('%Y-%m-%d-%H')
                    features['time_clusters'][hour_key].append(event)
                except Exception:
                    pass
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¨˜éŒ²
            pattern = f"{entity}:{rule}:{signature}"
            features['pattern_sequences'].append(pattern)
        
        return features
    
    def calculate_suppression_score(self, event: Dict, features: Dict) -> float:
        """ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ‘åˆ¶ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆé«˜ã‚¹ã‚³ã‚¢ = æŠ‘åˆ¶ã™ã¹ãï¼‰"""
        score = 0.0
        
        entity = event.get('entity_id', 'unknown')
        rule = event.get('rule_id', 'unknown')
        signature = event.get('signature', 'unknown')
        severity = event.get('severity', 'low')
        
        # 1. é »åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢
        entity_freq = features['entity_counts'][entity]
        rule_freq = features['rule_counts'][rule]
        _ = features['signature_counts'][signature]
        
        total_events = sum(features['entity_counts'].values())
        
        # é«˜é »åº¦ã‚¤ãƒ™ãƒ³ãƒˆã¯æŠ‘åˆ¶å€™è£œ
        if entity_freq > total_events * 0.1:  # 10%ä»¥ä¸Šã®é »åº¦
            score += self.feature_weights['entity_frequency'] * (entity_freq / total_events)
        
        if rule_freq > total_events * 0.08:
            score += self.feature_weights['rule_frequency'] * (rule_freq / total_events)
        
        # 2. æ™‚é–“çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        # çŸ­æ™‚é–“ã«é›†ä¸­ã—ã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã¯æŠ‘åˆ¶å€™è£œ
        pattern = f"{entity}:{rule}:{signature}"
        time_concentration = self._calculate_time_concentration(pattern, features)
        score += self.feature_weights['time_clustering'] * time_concentration
        
        # 3. ã‚·ã‚°ãƒãƒãƒ£ã®é¡ä¼¼æ€§
        # é¡ä¼¼ã‚·ã‚°ãƒãƒãƒ£ãŒå¤šã„å ´åˆã¯æŠ‘åˆ¶å€™è£œ
        similar_count = self._count_similar_signatures(signature, features['signature_counts'])
        if similar_count > 3:
            score += self.feature_weights['signature_similarity'] * min(1.0, similar_count / 10)
        
        # 4. é‡è¦åº¦ã«ã‚ˆã‚‹èª¿æ•´
        severity_scores = {'low': 0.8, 'medium': 0.5, 'high': 0.2, 'critical': 0.1}
        severity_multiplier = severity_scores.get(str(severity).lower(), 0.5)
        score *= severity_multiplier  # é‡è¦åº¦ãŒé«˜ã„ã‚¤ãƒ™ãƒ³ãƒˆã¯æŠ‘åˆ¶ã‚’æ§ãˆã‚‹
        
        # 5. å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹èª¿æ•´
        if pattern in self.learned_patterns:
            learned_score = self.learned_patterns[pattern]
            score = score * 0.7 + learned_score * 0.3  # å­¦ç¿’çµæœã‚’30%åæ˜ 
        
        return min(1.0, max(0.0, score))  # 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
    
    def _calculate_time_concentration(self, pattern: str, features: Dict) -> float:
        """æ™‚é–“çš„é›†ä¸­åº¦ã‚’è¨ˆç®—"""
        pattern_times = []
        
        for hour_key, events in features['time_clusters'].items():
            count = sum(1 for e in events if f"{e.get('entity_id')}:{e.get('rule_id')}:{e.get('signature')}" == pattern)
            if count > 0:
                pattern_times.append(count)
        
        if not pattern_times:
            return 0.0
        
        # åˆ†æ•£ãŒå°ã•ã„ï¼ˆé›†ä¸­ã—ã¦ã„ã‚‹ï¼‰ã»ã©é«˜ã‚¹ã‚³ã‚¢
        if len(pattern_times) == 1:
            return 1.0
        
        mean = np.mean(pattern_times)
        std = np.std(pattern_times)
        
        if mean == 0:
            return 0.0
        
        # å¤‰å‹•ä¿‚æ•°ã®é€†æ•°ï¼ˆé›†ä¸­åº¦ï¼‰
        concentration = 1.0 / (1.0 + std / mean)
        return concentration
    
    def _count_similar_signatures(self, signature: str, signature_counts: Counter) -> int:
        """é¡ä¼¼ã‚·ã‚°ãƒãƒãƒ£ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        similar_count = 0
        sig_lower = signature.lower()
        
        for other_sig in signature_counts:
            if other_sig != signature:
                # ç°¡å˜ãªé¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå…±é€šå˜èªæ•°ï¼‰
                words1 = set(sig_lower.replace('_', ' ').split())
                words2 = set(other_sig.lower().replace('_', ' ').split())
                
                if words1 and words2:
                    similarity = len(words1.intersection(words2)) / max(len(words1), len(words2))
                    if similarity > 0.5:
                        similar_count += 1
        
        return similar_count
    
    def optimize_suppression(self, events: List[Dict], target_reduction: float = 0.6) -> Tuple[List[Dict], List[Dict], Dict]:
        """æœ€é©ãªæŠ‘åˆ¶ã‚’å®Ÿè¡Œ"""
        features = self.extract_features(events)
        
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã«æŠ‘åˆ¶ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scored_events = []
        for event in events:
            score = self.calculate_suppression_score(event, features)
            scored_events.append((score, event))
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã‚¹ã‚³ã‚¢ã‹ã‚‰ï¼‰
        scored_events.sort(key=lambda x: x[0], reverse=True)
        
        # ç›®æ¨™å‰Šæ¸›ç‡ã‚’é”æˆã™ã‚‹ã¾ã§æŠ‘åˆ¶
        target_suppress_count = int(len(events) * target_reduction)
        
        suppressed_events = []
        passed_events = []
        
        # ã‚¹ã‚³ã‚¢é–¾å€¤ã‚’å‹•çš„ã«èª¿æ•´
        score_threshold = 0.05 if target_reduction > 0.5 else 0.3
        
        for i, (score, event) in enumerate(scored_events):
            if i < target_suppress_count and score > score_threshold:  # å‹•çš„é–¾å€¤ã§æŠ‘åˆ¶
                event['_suppression_score'] = score
                event['_suppressed'] = True
                suppressed_events.append(event)
            else:
                event['_suppression_score'] = score
                event['_suppressed'] = False
                passed_events.append(event)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
        for event in suppressed_events:
            pattern = f"{event.get('entity_id')}:{event.get('rule_id')}:{event.get('signature')}"
            if pattern in self.learned_patterns:
                # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰
                self.learned_patterns[pattern] = self.learned_patterns[pattern] * 0.8 + event['_suppression_score'] * 0.2
            else:
                self.learned_patterns[pattern] = event['_suppression_score']
        
        # çµ±è¨ˆæƒ…å ±
        actual_reduction = len(suppressed_events) / len(events) if events else 0
        
        stats = {
            'total_events': len(events),
            'suppressed_count': len(suppressed_events),
            'passed_count': len(passed_events),
            'actual_reduction_rate': actual_reduction,
            'target_reduction_rate': target_reduction,
            'average_suppression_score': np.mean([s for s, _ in scored_events]) if scored_events else 0,
            'learned_patterns': len(self.learned_patterns)
        }
        
        return passed_events, suppressed_events, stats
    
    def generate_advanced_rules(self, suppressed_events: List[Dict]) -> Dict:
        """æŠ‘åˆ¶ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰é«˜åº¦ãªãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ"""
        rules = {
            'composite_rules': [],
            'time_based_rules': [],
            'correlation_rules': []
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦ã‚’é›†è¨ˆ
        pattern_counter = Counter()
        time_patterns = defaultdict(list)
        
        for event in suppressed_events:
            pattern = f"{event.get('entity_id')}:{event.get('rule_id')}"
            pattern_counter[pattern] += 1
            
            if 'timestamp' in event:
                try:
                    ts = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
                    hour = ts.hour
                    time_patterns[pattern].append(hour)
                except Exception:
                    pass
        
        # 1. è¤‡åˆãƒ«ãƒ¼ãƒ«ï¼ˆé«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        for pattern, count in pattern_counter.most_common(10):
            if count >= 3:
                parts = pattern.split(':')
                rules['composite_rules'].append({
                    'entity': parts[0] if parts else '*',
                    'rule': parts[1] if len(parts) > 1 else '*',
                    'threshold': max(2, count // 2),
                    'window': 3600,
                    'action': 'suppress'
                })
        
        # 2. æ™‚é–“ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ«
        for pattern, hours in time_patterns.items():
            if len(hours) >= 5:
                # ç‰¹å®šã®æ™‚é–“å¸¯ã«é›†ä¸­ã—ã¦ã„ã‚‹å ´åˆ
                hour_counts = Counter(hours)
                peak_hour = hour_counts.most_common(1)[0][0]
                
                if hour_counts[peak_hour] >= len(hours) * 0.4:  # 40%ä»¥ä¸ŠãŒç‰¹å®šæ™‚é–“
                    parts = pattern.split(':')
                    rules['time_based_rules'].append({
                        'entity': parts[0] if parts else '*',
                        'rule': parts[1] if len(parts) > 1 else '*',
                        'peak_hours': [peak_hour, (peak_hour + 1) % 24],
                        'suppression_rate': 0.8,
                        'action': 'throttle'
                    })
        
        # 3. ç›¸é–¢ãƒ«ãƒ¼ãƒ«
        entity_groups = defaultdict(list)
        for event in suppressed_events:
            entity = event.get('entity_id', 'unknown')
            rule = event.get('rule_id', 'unknown')
            entity_groups[entity].append(rule)
        
        for entity, rules_list in entity_groups.items():
            if len(set(rules_list)) >= 3:  # 3ç¨®é¡ä»¥ä¸Šã®ãƒ«ãƒ¼ãƒ«
                rules['correlation_rules'].append({
                    'entity': entity,
                    'correlated_rules': list(set(rules_list))[:5],
                    'threshold': 5,
                    'window': 1800,
                    'action': 'group_suppress'
                })
        
        return rules


def main():
    parser = argparse.ArgumentParser(description='A/B Test Optimizer')
    parser.add_argument('--input', default='data/raw/events_splunk_2025-09-05_170644.jsonl',
                        help='Input events file')
    parser.add_argument('--output-passed', default='data/ab/B_optimized.jsonl',
                        help='Output file for passed events')
    parser.add_argument('--output-suppressed', default='data/ab/suppressed.jsonl',
                        help='Output file for suppressed events')
    parser.add_argument('--target-reduction', type=float, default=0.6,
                        help='Target reduction rate (0-1)')
    parser.add_argument('--rules-output', default='config/optimized_rules.json',
                        help='Output file for generated rules')
    
    args = parser.parse_args()
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
    optimizer = ABOptimizer()
    
    # ã‚¤ãƒ™ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
    events = []
    with open(args.input, 'r') as f:
        for line in f:
            if line.strip():
                events.append(json.loads(line))
    
    print(f"ğŸ“¥ {len(events)}ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    print(f"ğŸ¯ ç›®æ¨™å‰Šæ¸›ç‡: {args.target_reduction*100:.0f}%")
    
    # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
    print("\nğŸ§  æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’å®Ÿè¡Œä¸­...")
    passed_events, suppressed_events, stats = optimizer.optimize_suppression(events, args.target_reduction)
    
    # çµæœã‚’è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“Š æœ€é©åŒ–çµæœ")
    print("="*60)
    print(f"å…¥åŠ›ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {stats['total_events']}")
    print(f"æŠ‘åˆ¶ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {stats['suppressed_count']}")
    print(f"é€šéã‚¤ãƒ™ãƒ³ãƒˆæ•°: {stats['passed_count']}")
    print(f"é”æˆå‰Šæ¸›ç‡: {stats['actual_reduction_rate']*100:.1f}%")
    print(f"ç›®æ¨™ã¨ã®å·®: {(stats['actual_reduction_rate'] - stats['target_reduction_rate'])*100:+.1f}%")
    print(f"å¹³å‡æŠ‘åˆ¶ã‚¹ã‚³ã‚¢: {stats['average_suppression_score']:.3f}")
    print(f"å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {stats['learned_patterns']}")
    
    # ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä¿å­˜
    with open(args.output_passed, 'w') as f:
        for event in passed_events:
            cleaned = {k: v for k, v in event.items() if not k.startswith('_')}
            f.write(json.dumps(cleaned, ensure_ascii=False) + '\n')
    
    with open(args.output_suppressed, 'w') as f:
        for event in suppressed_events:
            f.write(json.dumps(event, ensure_ascii=False) + '\n')
    
    print("\nğŸ’¾ æœ€é©åŒ–æ¸ˆã¿ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä¿å­˜:")
    print(f"   é€šé: {args.output_passed}")
    print(f"   æŠ‘åˆ¶: {args.output_suppressed}")
    
    # é«˜åº¦ãªãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
    print("\nğŸ”§ é«˜åº¦ãªæŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆä¸­...")
    advanced_rules = optimizer.generate_advanced_rules(suppressed_events)
    
    with open(args.rules_output, 'w') as f:
        json.dump(advanced_rules, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ãƒ«ãƒ¼ãƒ«ã‚’ä¿å­˜: {args.rules_output}")
    print(f"   è¤‡åˆãƒ«ãƒ¼ãƒ«: {len(advanced_rules['composite_rules'])}å€‹")
    print(f"   æ™‚é–“ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ«: {len(advanced_rules['time_based_rules'])}å€‹")
    print(f"   ç›¸é–¢ãƒ«ãƒ¼ãƒ«: {len(advanced_rules['correlation_rules'])}å€‹")
    
    # é”æˆåº¦ã‚’è©•ä¾¡
    if stats['actual_reduction_rate'] >= args.target_reduction:
        print("\nğŸ‰ ç›®æ¨™å‰Šæ¸›ç‡ã‚’é”æˆã—ã¾ã—ãŸï¼")
    else:
        print(f"\nâš ï¸ ç›®æ¨™ã¾ã§ã‚ã¨{(args.target_reduction - stats['actual_reduction_rate'])*100:.1f}%ã§ã™")
        print("   â†’ ç”Ÿæˆã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€ã•ã‚‰ãªã‚‹æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™")
    
    return 0


if __name__ == '__main__':
    exit(main())
