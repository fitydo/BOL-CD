#!/usr/bin/env python3
"""
False Suppression Validator - èª¤æŠ‘åˆ¶ã®æ¤œè¨¼ã¨è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
èª¤æŠ‘åˆ¶ã‚’æ­£ç¢ºã«åˆ¤å®šã™ã‚‹ãŸã‚ã®å¤šå±¤çš„ãªæ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
"""
import json
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict, Counter
import numpy as np
from dataclasses import dataclass

@dataclass
class ValidationResult:
    """æ¤œè¨¼çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    total_suppressed: int
    confirmed_false_suppressions: int
    suspected_false_suppressions: int
    confidence_level: float
    validation_method: str
    details: Dict

class FalseSuppressionValidator:
    """èª¤æŠ‘åˆ¶ã‚’å¤šè§’çš„ã«æ¤œè¨¼ã™ã‚‹ãƒãƒªãƒ‡ãƒ¼ã‚¿"""
    
    def __init__(self, suppressed_file: str, passed_file: str, original_file: str = None):
        self.suppressed = self._load_events(suppressed_file)
        self.passed = self._load_events(passed_file)
        self.original = self._load_events(original_file) if original_file else []
        
        # æ¤œè¨¼çµæœã‚’æ ¼ç´
        self.validation_results = {}
        
        # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆåˆ¤å®šã®ãŸã‚ã®é‡è¦ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.incident_patterns = {
            'critical_auth': [
                'authentication.*failed.*admin',
                'privilege.*escalation',
                'unauthorized.*root.*access',
                'brute.*force.*detected',
                'account.*lockout.*multiple'
            ],
            'data_breach': [
                'data.*exfiltration',
                'sensitive.*data.*exposed',
                'database.*dump',
                'unauthorized.*download.*large',
                'confidential.*file.*accessed'
            ],
            'malware': [
                'malware.*detected',
                'ransomware.*activity',
                'trojan.*found',
                'virus.*signature.*matched',
                'suspicious.*executable'
            ],
            'network_attack': [
                'ddos.*attack',
                'port.*scan.*aggressive',
                'sql.*injection.*successful',
                'remote.*code.*execution',
                'zero.*day.*exploit'
            ]
        }
        
    def _load_events(self, file_path: str) -> List[Dict]:
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not file_path or not Path(file_path).exists():
            return []
        
        events = []
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip():
                    events.append(json.loads(line))
        return events
    
    def validate_by_severity_rules(self) -> ValidationResult:
        """
        æ–¹æ³•1: Severity/Signatureãƒ™ãƒ¼ã‚¹ã®æ¤œè¨¼
        High/Criticalã‚¤ãƒ™ãƒ³ãƒˆã®æŠ‘åˆ¶ã‚’èª¤æŠ‘åˆ¶ã¨ã—ã¦åˆ¤å®š
        """
        false_suppressions = []
        suspected = []
        
        for event in self.suppressed:
            severity = event.get('severity', 'unknown').lower()
            # signature not used presently; derive features from raw only
            raw = event.get('_raw', '').lower()
            
            # Critical/Highã¯åŸå‰‡èª¤æŠ‘åˆ¶
            if severity in ['critical', 'high']:
                false_suppressions.append(event)
            
            # ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆã¯ç–‘ã„
            danger_keywords = ['failed', 'denied', 'error', 'attack', 'malware', 
                             'unauthorized', 'breach', 'exploit', 'injection']
            if any(kw in raw for kw in danger_keywords):
                if event not in false_suppressions:
                    suspected.append(event)
        
        confidence = 0.9 if len(false_suppressions) == 0 else 0.7
        
        return ValidationResult(
            total_suppressed=len(self.suppressed),
            confirmed_false_suppressions=len(false_suppressions),
            suspected_false_suppressions=len(suspected),
            confidence_level=confidence,
            validation_method="Severity/Keyword Rules",
            details={
                'false_suppression_events': [e.get('event_id', e.get('rule_id')) for e in false_suppressions[:5]],
                'suspected_events': [e.get('event_id', e.get('rule_id')) for e in suspected[:5]]
            }
        )
    
    def validate_by_incident_correlation(self) -> ValidationResult:
        """
        æ–¹æ³•2: ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆç›¸é–¢ã«ã‚ˆã‚‹æ¤œè¨¼
        æŠ‘åˆ¶ã‚¤ãƒ™ãƒ³ãƒˆã®å¾Œã«é–¢é€£ã™ã‚‹é‡å¤§ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸã‹ãƒã‚§ãƒƒã‚¯
        """
        false_suppressions = []
        time_window = timedelta(hours=1)  # 1æ™‚é–“ä»¥å†…ã®ç›¸é–¢ã‚’è¦‹ã‚‹
        
        # æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
        all_events = self.suppressed + self.passed
        all_events.sort(key=lambda x: x.get('timestamp', ''))
        
        # å„æŠ‘åˆ¶ã‚¤ãƒ™ãƒ³ãƒˆã«ã¤ã„ã¦å¾Œç¶šã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        for supp_event in self.suppressed:
            supp_time = datetime.fromisoformat(supp_event.get('timestamp', datetime.now().isoformat()))
            supp_entity = supp_event.get('entity_id', '')
            
            # åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã§å¾Œç¶šã®é‡å¤§ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¢ã™
            for event in all_events:
                event_time = datetime.fromisoformat(event.get('timestamp', datetime.now().isoformat()))
                
                # æ™‚é–“çª“å†…ã§åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
                if (event_time > supp_time and 
                    event_time - supp_time <= time_window and
                    event.get('entity_id') == supp_entity):
                    
                    # é‡å¤§ã‚¤ãƒ™ãƒ³ãƒˆã‹ãƒã‚§ãƒƒã‚¯
                    if (event.get('severity') in ['critical', 'high'] or
                        self._is_incident_pattern(event)):
                        false_suppressions.append({
                            'suppressed_event': supp_event,
                            'correlated_incident': event,
                            'time_delta': str(event_time - supp_time)
                        })
                        break
        
        confidence = 0.85  # ç›¸é–¢ãƒ™ãƒ¼ã‚¹ãªã®ã§å°‘ã—æ§ãˆã‚
        
        return ValidationResult(
            total_suppressed=len(self.suppressed),
            confirmed_false_suppressions=len(false_suppressions),
            suspected_false_suppressions=0,
            confidence_level=confidence,
            validation_method="Incident Correlation",
            details={
                'correlated_pairs': len(false_suppressions),
                'examples': false_suppressions[:3]
            }
        )
    
    def _is_incident_pattern(self, event: Dict) -> bool:
        """ã‚¤ãƒ™ãƒ³ãƒˆãŒã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆè‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        raw = event.get('_raw', '').lower()
        
        for category, patterns in self.incident_patterns.items():
            for pattern in patterns:
                # ç°¡æ˜“çš„ãªæ­£è¦è¡¨ç¾ãƒãƒƒãƒãƒ³ã‚°
                keywords = pattern.split('.*')
                if all(kw in raw for kw in keywords):
                    return True
        return False
    
    def validate_by_statistical_anomaly(self) -> ValidationResult:
        """
        æ–¹æ³•3: çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹æ¤œè¨¼
        ãƒ¬ã‚¢ã‚¤ãƒ™ãƒ³ãƒˆã‚„ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ‘åˆ¶ã‚’æ¤œå‡º
        """
        false_suppressions = []
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã”ã¨ã®ã‚¤ãƒ™ãƒ³ãƒˆé »åº¦ã‚’è¨ˆç®—
        entity_counts = Counter()
        for event in self.passed + self.suppressed:
            entity_counts[event.get('entity_id')] += 1
        
        # å¹³å‡ã¨æ¨™æº–åå·®
        counts = list(entity_counts.values())
        if counts:
            mean_count = np.mean(counts)
            std_count = np.std(counts)
            
            # ãƒ¬ã‚¢ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆå¹³å‡-2Ïƒä»¥ä¸‹ï¼‰ã®ã‚¤ãƒ™ãƒ³ãƒˆæŠ‘åˆ¶ã‚’ãƒã‚§ãƒƒã‚¯
            rare_threshold = max(1, mean_count - 2 * std_count)
            
            for event in self.suppressed:
                entity = event.get('entity_id')
                if entity_counts[entity] <= rare_threshold:
                    false_suppressions.append({
                        'event': event,
                        'reason': 'rare_entity',
                        'entity_frequency': entity_counts[entity],
                        'threshold': rare_threshold
                    })
        
        # ãƒãƒ¼ã‚¹ãƒˆæ¤œçŸ¥ï¼ˆçŸ­æ™‚é–“ã«é›†ä¸­ï¼‰
        time_buckets = defaultdict(list)
        for event in self.suppressed:
            timestamp = event.get('timestamp', '')
            if timestamp:
                # 10åˆ†å˜ä½ã§ãƒã‚±ãƒƒãƒˆåŒ–
                bucket = timestamp[:15] + '0:00'
                time_buckets[bucket].append(event)
        
        # ãƒãƒ¼ã‚¹ãƒˆåˆ¤å®šï¼ˆ10åˆ†ã§5ä»¶ä»¥ä¸Šï¼‰
        for bucket, events in time_buckets.items():
            if len(events) >= 5:
                for event in events:
                    if not any(fs['event'] == event for fs in false_suppressions):
                        false_suppressions.append({
                            'event': event,
                            'reason': 'burst_pattern',
                            'burst_size': len(events),
                            'time_bucket': bucket
                        })
        
        confidence = 0.75  # çµ±è¨ˆçš„æ‰‹æ³•ãªã®ã§ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦
        
        return ValidationResult(
            total_suppressed=len(self.suppressed),
            confirmed_false_suppressions=len(false_suppressions),
            suspected_false_suppressions=0,
            confidence_level=confidence,
            validation_method="Statistical Anomaly",
            details={
                'rare_entity_suppressions': sum(1 for fs in false_suppressions if fs['reason'] == 'rare_entity'),
                'burst_suppressions': sum(1 for fs in false_suppressions if fs['reason'] == 'burst_pattern')
            }
        )
    
    def validate_by_shadow_mode(self, shadow_results: Optional[Dict] = None) -> ValidationResult:
        """
        æ–¹æ³•4: Shadow Modeã«ã‚ˆã‚‹æ¤œè¨¼
        å®Ÿéš›ã®SOCã‚¢ãƒŠãƒªã‚¹ãƒˆã®åˆ¤å®šã¨æ¯”è¼ƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        """
        if not shadow_results:
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼šãƒ©ãƒ³ãƒ€ãƒ ã«10%ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦äººæ‰‹åˆ¤å®šã‚’æ¨¡æ“¬
            sample_size = max(1, len(self.suppressed) // 10)
            sampled = random.sample(self.suppressed, min(sample_size, len(self.suppressed)))
            
            # å®Ÿéš›ã«ã¯SOCã‚¢ãƒŠãƒªã‚¹ãƒˆã®åˆ¤å®šãŒå¿…è¦
            # ã“ã“ã§ã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§æ¨¡æ“¬
            shadow_results = {}
            for event in sampled:
                # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿéš›ã¯äººæ‰‹ï¼‰
                is_false = (
                    event.get('severity') in ['critical', 'high'] or
                    'error' in event.get('_raw', '').lower() or
                    self._is_incident_pattern(event)
                )
                shadow_results[event.get('rule_id', '')] = {
                    'should_pass': is_false,
                    'analyst_confidence': 0.9 if is_false else 0.8
                }
        
        # Shadowçµæœã‹ã‚‰èª¤æŠ‘åˆ¶ã‚’é›†è¨ˆ
        false_suppressions = []
        for event in self.suppressed:
            rule_id = event.get('rule_id', '')
            if rule_id in shadow_results and shadow_results[rule_id]['should_pass']:
                false_suppressions.append(event)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‹ã‚‰å…¨ä½“ã‚’æ¨å®š
        sample_rate = len(shadow_results) / max(1, len(self.suppressed))
        estimated_false = int(len(false_suppressions) / sample_rate) if sample_rate > 0 else 0
        
        # ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—ï¼ˆClopper-Pearsonï¼‰
        n_samples = len(shadow_results)
        n_false = len(false_suppressions)
        if n_samples > 0:
            # ç°¡æ˜“çš„ãª95%ä¿¡é ¼åŒºé–“
            p = n_false / n_samples
            margin = 1.96 * np.sqrt(p * (1 - p) / n_samples)
            ci_lower = max(0, p - margin)
            ci_upper = min(1, p + margin)
        else:
            ci_lower, ci_upper = 0, 1
        
        confidence = 0.95 if n_samples >= 30 else 0.8
        
        return ValidationResult(
            total_suppressed=len(self.suppressed),
            confirmed_false_suppressions=len(false_suppressions),
            suspected_false_suppressions=estimated_false - len(false_suppressions),
            confidence_level=confidence,
            validation_method="Shadow Mode (Sampled)",
            details={
                'sample_size': n_samples,
                'sample_false_suppressions': n_false,
                'estimated_total_false': estimated_false,
                'confidence_interval': f"[{ci_lower*100:.1f}%, {ci_upper*100:.1f}%]"
            }
        )
    
    def validate_all_methods(self) -> Dict:
        """
        ã™ã¹ã¦ã®æ¤œè¨¼æ–¹æ³•ã‚’å®Ÿè¡Œã—ã¦ç·åˆåˆ¤å®š
        """
        results = {}
        
        print("ğŸ” èª¤æŠ‘åˆ¶ã®å¤šå±¤çš„æ¤œè¨¼ã‚’é–‹å§‹...")
        print("="*70)
        
        # å„æ¤œè¨¼æ–¹æ³•ã‚’å®Ÿè¡Œ
        methods = [
            ('severity_rules', self.validate_by_severity_rules),
            ('incident_correlation', self.validate_by_incident_correlation),
            ('statistical_anomaly', self.validate_by_statistical_anomaly),
            ('shadow_mode', self.validate_by_shadow_mode)
        ]
        
        for name, method in methods:
            print(f"\nğŸ“Š æ¤œè¨¼æ–¹æ³•: {name}")
            result = method()
            results[name] = result
            
            print(f"  æŠ‘åˆ¶ç·æ•°: {result.total_suppressed}")
            print(f"  ç¢ºå®šèª¤æŠ‘åˆ¶: {result.confirmed_false_suppressions}")
            print(f"  ç–‘ã„èª¤æŠ‘åˆ¶: {result.suspected_false_suppressions}")
            print(f"  ä¿¡é ¼åº¦: {result.confidence_level:.1%}")
            
            # è©³ç´°è¡¨ç¤º
            if result.details:
                for key, value in result.details.items():
                    if isinstance(value, list) and len(value) > 0:
                        print(f"  {key}: {value[:3]}...")
                    else:
                        print(f"  {key}: {value}")
        
        # ç·åˆåˆ¤å®š
        all_confirmed = sum(r.confirmed_false_suppressions for r in results.values())
        _ = sum(r.suspected_false_suppressions for r in results.values())
        avg_confidence = np.mean([r.confidence_level for r in results.values()])
        
        # é‡ã¿ä»˜ãå¹³å‡ï¼ˆä¿¡é ¼åº¦ã§é‡ã¿ä»˜ã‘ï¼‰
        weighted_false = sum(
            r.confirmed_false_suppressions * r.confidence_level 
            for r in results.values()
        ) / sum(r.confidence_level for r in results.values())
        
        print("\n" + "="*70)
        print("ğŸ“ˆ ç·åˆåˆ¤å®šçµæœ:")
        print(f"  æ¤œè¨¼æ–¹æ³•æ•°: {len(results)}")
        print(f"  å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.1%}")
        print(f"  æœ€å¤§èª¤æŠ‘åˆ¶æ•°: {all_confirmed}")
        print(f"  é‡ã¿ä»˜ãèª¤æŠ‘åˆ¶æ¨å®š: {weighted_false:.1f}")
        
        # æœ€çµ‚çš„ãªèª¤æŠ‘åˆ¶ç‡
        if self.suppressed:
            false_suppression_rate = weighted_false / len(self.suppressed)
            print(f"\nğŸ¯ æ¨å®šèª¤æŠ‘åˆ¶ç‡: {false_suppression_rate:.2%}")
            
            if false_suppression_rate < 0.01:
                print("  âœ… èª¤æŠ‘åˆ¶ç‡ã¯è¨±å®¹ç¯„å›²å†…ã§ã™ï¼ˆ<1%ï¼‰")
            elif false_suppression_rate < 0.05:
                print("  âš ï¸ èª¤æŠ‘åˆ¶ç‡ãŒã‚„ã‚„é«˜ã‚ã§ã™ï¼ˆ1-5%ï¼‰")
            else:
                print("  âŒ èª¤æŠ‘åˆ¶ç‡ãŒé«˜ã™ãã¾ã™ï¼ˆ>5%ï¼‰- èª¿æ•´ãŒå¿…è¦")
        
        return {
            'individual_results': results,
            'summary': {
                'total_suppressed': len(self.suppressed),
                'weighted_false_suppressions': weighted_false,
                'estimated_false_rate': false_suppression_rate if self.suppressed else 0,
                'average_confidence': avg_confidence,
                'recommendation': self._get_recommendation(false_suppression_rate if self.suppressed else 0)
            }
        }
    
    def _get_recommendation(self, false_rate: float) -> str:
        """èª¤æŠ‘åˆ¶ç‡ã«åŸºã¥ãæ¨å¥¨äº‹é …"""
        if false_rate < 0.01:
            return "ç¾åœ¨ã®è¨­å®šã¯é©åˆ‡ã§ã™ã€‚é‹ç”¨ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚"
        elif false_rate < 0.05:
            return "High/Criticalã‚¤ãƒ™ãƒ³ãƒˆã®æŠ‘åˆ¶åŸºæº–ã‚’è¦‹ç›´ã™ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        else:
            return "æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«ã®å¤§å¹…ãªè¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚Shadow Modeã§ã®é‹ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
    
    def export_validation_report(self, output_file: str = "reports/false_suppression_validation.json"):
        """æ¤œè¨¼çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦å‡ºåŠ›"""
        results = self.validate_all_methods()
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'input_files': {
                'suppressed_count': len(self.suppressed),
                'passed_count': len(self.passed)
            },
            'validation_results': results,
            'metadata': {
                'validator_version': '1.0.0',
                'methods_used': list(results['individual_results'].keys())
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nğŸ“„ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")
        return output_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="èª¤æŠ‘åˆ¶ã®æ¤œè¨¼")
    parser.add_argument('--suppressed', required=True, help='æŠ‘åˆ¶ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--passed', required=True, help='é€šéã—ãŸã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--original', help='å…ƒã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰')
    parser.add_argument('--output', default='reports/false_suppression_validation.json', help='å‡ºåŠ›ãƒ¬ãƒãƒ¼ãƒˆ')
    args = parser.parse_args()
    
    validator = FalseSuppressionValidator(args.suppressed, args.passed, args.original)
    validator.export_validation_report(args.output)

if __name__ == "__main__":
    main()
