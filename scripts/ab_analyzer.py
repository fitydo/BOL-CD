#!/usr/bin/env python3
"""
A/B Test Analyzer - ãƒ‡ãƒ¼ã‚¿è§£æã¨å‰Šæ¸›ç‡å‘ä¸Šã®ãŸã‚ã®åˆ†æãƒ„ãƒ¼ãƒ«
"""
import json
import argparse
from pathlib import Path
from collections import Counter
from typing import Dict, List
from datetime import datetime

class ABAnalyzer:
    def __init__(self, data_dir: str = "data/ab", reports_dir: str = "reports"):
        self.data_dir = Path(data_dir)
        self.reports_dir = Path(reports_dir)
        self.analysis_results = {}
        
    def load_events(self, file_path: str) -> List[Dict]:
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        events = []
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip():
                    events.append(json.loads(line))
        return events
    
    def analyze_patterns(self, events_a: List[Dict], events_b: List[Dict]) -> Dict:
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        # Aç¾¤ã¨Bç¾¤ã®ç‰¹å¾´ã‚’æŠ½å‡º
        patterns_a = self._extract_patterns(events_a)
        patterns_b = self._extract_patterns(events_b)
        
        # å‰Šæ¸›ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
        reduced_patterns = []
        for pattern, count_a in patterns_a.items():
            count_b = patterns_b.get(pattern, 0)
            if count_b < count_a:
                reduction_rate = (count_a - count_b) / count_a if count_a > 0 else 0
                reduced_patterns.append({
                    'pattern': pattern,
                    'count_a': count_a,
                    'count_b': count_b,
                    'reduction_rate': reduction_rate,
                    'impact': count_a - count_b  # å‰Šæ¸›æ•°
                })
        
        # å‰Šæ¸›ç‡ãŒä½ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šï¼ˆæ”¹å–„ä½™åœ°ã‚ã‚Šï¼‰
        low_reduction_patterns = [
            p for p in reduced_patterns 
            if p['reduction_rate'] < 0.3 and p['count_a'] > 5
        ]
        
        # Bç¾¤ã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ï¼‰
        new_in_b = []
        for pattern, count_b in patterns_b.items():
            if pattern not in patterns_a and count_b > 0:
                new_in_b.append({
                    'pattern': pattern,
                    'count': count_b,
                    'severity': self._estimate_severity(pattern)
                })
        
        return {
            'total_patterns_a': len(patterns_a),
            'total_patterns_b': len(patterns_b),
            'reduced_patterns': sorted(reduced_patterns, key=lambda x: x['impact'], reverse=True)[:20],
            'low_reduction_patterns': sorted(low_reduction_patterns, key=lambda x: x['count_a'], reverse=True)[:10],
            'new_in_b': sorted(new_in_b, key=lambda x: x['count'], reverse=True)[:10],
            'overall_reduction': self._calculate_overall_reduction(events_a, events_b)
        }
    
    def _extract_patterns(self, events: List[Dict]) -> Dict[str, int]:
        """ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        patterns = Counter()
        
        for event in events:
            # è¤‡æ•°ã®ç‰¹å¾´ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
            entity = event.get('entity_id', 'unknown')
            rule = event.get('rule_id', 'unknown')
            signature = event.get('signature', 'unknown')
            severity = event.get('severity', 'unknown')
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ + ãƒ«ãƒ¼ãƒ«
            patterns[f"{entity}:{rule}"] += 1
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚·ã‚°ãƒãƒãƒ£ + é‡è¦åº¦
            patterns[f"{signature}:{severity}"] += 1
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ + ã‚·ã‚°ãƒãƒãƒ£
            patterns[f"{entity}:{signature}"] += 1
            
        return dict(patterns)
    
    def _estimate_severity(self, pattern: str) -> str:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¦åº¦ã‚’æ¨å®š"""
        critical_keywords = ['critical', 'high', 'escalation', 'malware', 'exfiltration']
        medium_keywords = ['medium', 'unauthorized', 'failed', 'injection']
        
        pattern_lower = pattern.lower()
        for keyword in critical_keywords:
            if keyword in pattern_lower:
                return 'critical'
        for keyword in medium_keywords:
            if keyword in pattern_lower:
                return 'medium'
        return 'low'
    
    def _calculate_overall_reduction(self, events_a: List[Dict], events_b: List[Dict]) -> float:
        """å…¨ä½“ã®å‰Šæ¸›ç‡ã‚’è¨ˆç®—"""
        count_a = len(events_a)
        count_b = len(events_b)
        if count_a == 0:
            return 0.0
        return (count_a - count_b) / count_a
    
    def suggest_optimizations(self, analysis: Dict) -> List[Dict]:
        """æœ€é©åŒ–ã®ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        
        # 1. ä½å‰Šæ¸›ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ”¹å–„ææ¡ˆ
        for pattern in analysis['low_reduction_patterns'][:5]:
            suggestions.append({
                'type': 'threshold_adjustment',
                'pattern': pattern['pattern'],
                'current_reduction': pattern['reduction_rate'],
                'target_reduction': 0.6,
                'action': f"é–¾å€¤ã‚’èª¿æ•´: {pattern['pattern']} ã®é‡è¤‡åˆ¤å®šåŸºæº–ã‚’ç·©å’Œ",
                'expected_impact': pattern['count_a'] * 0.3  # 30%è¿½åŠ å‰Šæ¸›ã‚’æœŸå¾…
            })
        
        # 2. é«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«è¿½åŠ 
        for pattern in analysis['reduced_patterns'][:5]:
            if pattern['reduction_rate'] < 0.5:
                suggestions.append({
                    'type': 'suppression_rule',
                    'pattern': pattern['pattern'],
                    'current_count': pattern['count_b'],
                    'action': f"æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«è¿½åŠ : {pattern['pattern']} ã®é€£ç¶šç™ºç”Ÿã‚’æŠ‘åˆ¶",
                    'expected_impact': pattern['count_b'] * 0.5
                })
        
        # 3. ãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®å¯¾å‡¦
        for pattern in analysis['new_in_b'][:3]:
            if pattern['severity'] != 'critical':
                suggestions.append({
                    'type': 'false_positive_filter',
                    'pattern': pattern['pattern'],
                    'action': f"èª¤æ¤œçŸ¥ãƒ•ã‚£ãƒ«ã‚¿: {pattern['pattern']} ã‚’é™¤å¤–å€™è£œã«è¿½åŠ ",
                    'expected_impact': pattern['count']
                })
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—
        total_expected_reduction = sum(s.get('expected_impact', 0) for s in suggestions)
        current_total = len(analysis.get('events_a', [])) if 'events_a' in analysis else 1000
        expected_reduction_rate = analysis['overall_reduction'] + (total_expected_reduction / current_total)
        
        return {
            'suggestions': suggestions,
            'current_reduction_rate': analysis['overall_reduction'],
            'expected_reduction_rate': min(expected_reduction_rate, 0.8),  # æœ€å¤§80%
            'total_expected_impact': total_expected_reduction
        }
    
    def generate_tuning_config(self, suggestions: Dict) -> Dict:
        """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’ç”Ÿæˆ"""
        config = {
            'timestamp': datetime.now().isoformat(),
            'target_reduction_rate': 0.6,
            'threshold_adjustments': {},
            'suppression_rules': [],
            'false_positive_filters': []
        }
        
        for suggestion in suggestions['suggestions']:
            if suggestion['type'] == 'threshold_adjustment':
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é–¾å€¤èª¿æ•´å¯¾è±¡ã‚’æŠ½å‡º
                parts = suggestion['pattern'].split(':')
                if len(parts) >= 2:
                    config['threshold_adjustments'][parts[0]] = {
                        'rule': parts[1],
                        'multiplier': 0.7,  # é–¾å€¤ã‚’30%ç·©å’Œ
                        'min_interval': 300  # 5åˆ†é–“ã®æœ€å°é–“éš”
                    }
            
            elif suggestion['type'] == 'suppression_rule':
                parts = suggestion['pattern'].split(':')
                config['suppression_rules'].append({
                    'pattern': suggestion['pattern'],
                    'entity': parts[0] if parts else '*',
                    'rule': parts[1] if len(parts) > 1 else '*',
                    'window': 3600,  # 1æ™‚é–“ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                    'threshold': 3,  # 3å›ä»¥ä¸Šã§æŠ‘åˆ¶
                    'suppress_duration': 1800  # 30åˆ†é–“æŠ‘åˆ¶
                })
            
            elif suggestion['type'] == 'false_positive_filter':
                config['false_positive_filters'].append({
                    'pattern': suggestion['pattern'],
                    'confidence': 0.8,
                    'auto_suppress': True
                })
        
        return config
    
    def run_analysis(self) -> Dict:
        """å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œ"""
        # A/Bãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        a_file = self.data_dir / "A.jsonl"
        b_file = self.data_dir / "B.jsonl"
        
        if not a_file.exists() or not b_file.exists():
            return {'error': 'A/B data files not found'}
        
        events_a = self.load_events(a_file)
        events_b = self.load_events(b_file)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        analysis = self.analyze_patterns(events_a, events_b)
        analysis['events_a'] = events_a  # å¾Œã®è¨ˆç®—ç”¨
        
        # æœ€é©åŒ–ææ¡ˆ
        optimizations = self.suggest_optimizations(analysis)
        
        # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šç”Ÿæˆ
        tuning_config = self.generate_tuning_config(optimizations)
        
        return {
            'analysis': analysis,
            'optimizations': optimizations,
            'tuning_config': tuning_config,
            'summary': {
                'current_reduction': f"{analysis['overall_reduction']*100:.1f}%",
                'expected_reduction': f"{optimizations['expected_reduction_rate']*100:.1f}%",
                'improvement': f"{(optimizations['expected_reduction_rate'] - analysis['overall_reduction'])*100:.1f}%",
                'suggestions_count': len(optimizations['suggestions'])
            }
        }


def main():
    parser = argparse.ArgumentParser(description='A/B Test Analyzer')
    parser.add_argument('--data-dir', default='data/ab', help='A/B data directory')
    parser.add_argument('--reports-dir', default='reports', help='Reports directory')
    parser.add_argument('--output', help='Output file for analysis results')
    
    args = parser.parse_args()
    
    analyzer = ABAnalyzer(args.data_dir, args.reports_dir)
    results = analyzer.run_analysis()
    
    # çµæœã‚’è¡¨ç¤º
    if 'error' in results:
        print(f"âŒ Error: {results['error']}")
        return 1
    
    print("\n" + "="*60)
    print("ğŸ“Š A/Bå‰Šæ¸›ç‡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*60)
    
    summary = results['summary']
    print(f"\nç¾åœ¨ã®å‰Šæ¸›ç‡: {summary['current_reduction']}")
    print(f"æœŸå¾…å‰Šæ¸›ç‡: {summary['expected_reduction']}")
    print(f"æ”¹å–„å¹…: {summary['improvement']}")
    print(f"ææ¡ˆæ•°: {summary['suggestions_count']}")
    
    print("\nğŸ¯ æ”¹å–„ææ¡ˆTOP5:")
    for i, suggestion in enumerate(results['optimizations']['suggestions'][:5], 1):
        print(f"\n{i}. {suggestion['type']}")
        print(f"   å¯¾è±¡: {suggestion['pattern']}")
        print(f"   ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {suggestion['action']}")
        print(f"   æœŸå¾…åŠ¹æœ: {suggestion.get('expected_impact', 0):.0f}ä»¶å‰Šæ¸›")
    
    # çµæœã‚’ä¿å­˜
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = Path(args.reports_dir) / f"ab_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è©³ç´°åˆ†æçµæœã‚’ä¿å­˜: {output_file}")
    
    # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’ä¿å­˜
    tuning_file = Path("config") / "tuning_config.json"
    tuning_file.parent.mkdir(exist_ok=True)
    with open(tuning_file, 'w') as f:
        json.dump(results['tuning_config'], f, indent=2, ensure_ascii=False)
    
    print(f"âš™ï¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’ä¿å­˜: {tuning_file}")
    
    return 0


if __name__ == '__main__':
    exit(main())
