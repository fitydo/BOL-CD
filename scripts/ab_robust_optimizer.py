#!/usr/bin/env python3
"""
Robust A/B Optimizer - å†ç¾æ€§ã®ã‚ã‚‹å‰Šæ¸›ç‡æœ€é©åŒ–
å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€äº¤å·®æ¤œè¨¼ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½ã®å‘ä¸Š
"""
import json
import argparse
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import Counter, defaultdict
from datetime import datetime
import numpy as np
from dataclasses import dataclass

@dataclass
class OptimizationConfig:
    """æœ€é©åŒ–ã®å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    # ç‰¹å¾´é‡ã®é‡ã¿ï¼ˆå›ºå®šï¼‰
    feature_weights = {
        'frequency': 0.4,
        'burstiness': 0.3,
        'pattern_similarity': 0.2,
        'severity_factor': 0.1
    }
    
    # æŠ‘åˆ¶ã‚¹ã‚³ã‚¢ã®é–¾å€¤ï¼ˆå›ºå®šï¼‰
    suppression_threshold = 0.25
    
    # å­¦ç¿’ç‡ï¼ˆå›ºå®šï¼‰
    learning_rate = 0.1
    
    # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
    min_samples = 5
    
    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆç§’ï¼‰
    time_window = 3600
    
    # ãƒãƒ¼ã‚¹ãƒˆæ¤œå‡ºé–¾å€¤
    burst_threshold = 3.0

class RobustOptimizer:
    def __init__(self, config: OptimizationConfig = None):
        self.config = config or OptimizationConfig()
        self.models = []  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ã®ãƒ¢ãƒ‡ãƒ«ç¾¤
        self.feature_cache = {}
        self.validation_scores = []
        
    def extract_robust_features(self, events: List[Dict]) -> np.ndarray:
        """å …ç‰¢ãªç‰¹å¾´é‡æŠ½å‡º"""
        features_list = []
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã®ç‰¹å¾´ã‚’è¨ˆç®—
        for event in events:
            features = self._compute_event_features(event, events)
            features_list.append(features)
        
        return np.array(features_list)
    
    def _compute_event_features(self, event: Dict, all_events: List[Dict]) -> np.ndarray:
        """å˜ä¸€ã‚¤ãƒ™ãƒ³ãƒˆã®ç‰¹å¾´é‡è¨ˆç®—"""
        features = []
        
        # 1. é »åº¦ç‰¹å¾´
        entity = event.get('entity_id', '')
        rule = event.get('rule_id', '')
        pattern = f"{entity}:{rule}"
        
        pattern_count = sum(1 for e in all_events 
                          if f"{e.get('entity_id')}:{e.get('rule_id')}" == pattern)
        freq_score = min(1.0, pattern_count / max(1, len(all_events)))
        features.append(freq_score)
        
        # 2. ãƒãƒ¼ã‚¹ãƒˆæ€§ï¼ˆæ™‚é–“çš„é›†ä¸­åº¦ï¼‰
        burst_score = self._compute_burstiness(event, all_events)
        features.append(burst_score)
        
        # 3. ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦
        similarity_score = self._compute_pattern_similarity(event, all_events)
        features.append(similarity_score)
        
        # 4. é‡è¦åº¦ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆä½ã„æ–¹ãŒæŠ‘åˆ¶ã—ã‚„ã™ã„ï¼‰
        severity_map = {'critical': 0.1, 'high': 0.3, 'medium': 0.5, 'low': 0.8}
        severity = event.get('severity', 'medium')
        severity_score = severity_map.get(str(severity).lower(), 0.5)
        features.append(severity_score)
        
        # 5. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆå¤šæ§˜æ€§ã®æŒ‡æ¨™ï¼‰
        entropy_score = self._compute_entropy(pattern, all_events)
        features.append(entropy_score)
        
        return np.array(features)
    
    def _compute_burstiness(self, event: Dict, all_events: List[Dict]) -> float:
        """ãƒãƒ¼ã‚¹ãƒˆæ€§ã®è¨ˆç®—ï¼ˆKlein's burstiness measureï¼‰"""
        pattern = f"{event.get('entity_id')}:{event.get('rule_id')}"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾é–“éš”ã‚’è¨ˆç®—
        intervals = []
        pattern_events = [e for e in all_events 
                         if f"{e.get('entity_id')}:{e.get('rule_id')}" == pattern]
        
        if len(pattern_events) < 2:
            return 0.0
        
        # ç°¡æ˜“çš„ãªæ™‚é–“å·®è¨ˆç®—ï¼ˆå®Ÿéš›ã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰è¨ˆç®—ï¼‰
        for i in range(1, len(pattern_events)):
            intervals.append(1.0)  # ä»®ã®é–“éš”
        
        if not intervals:
            return 0.0
        
        mean_interval = np.mean(intervals)
        std_interval = np.std(intervals)
        
        if mean_interval == 0:
            return 1.0
        
        # ãƒãƒ¼ã‚¹ãƒˆæ€§ = (æ¨™æº–åå·® - å¹³å‡) / (æ¨™æº–åå·® + å¹³å‡)
        burstiness = (std_interval - mean_interval) / (std_interval + mean_interval + 1e-6)
        return max(0.0, min(1.0, (burstiness + 1.0) / 2.0))
    
    def _compute_pattern_similarity(self, event: Dict, all_events: List[Dict]) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—"""
        signature = event.get('signature', '')
        if not signature:
            return 0.0
        
        similar_count = 0
        sig_tokens = set(signature.lower().replace('_', ' ').split())
        
        for other in all_events[:100]:  # è¨ˆç®—é‡å‰Šæ¸›ã®ãŸã‚æœ€åˆã®100ä»¶ã®ã¿
            other_sig = other.get('signature', '')
            if other_sig and other_sig != signature:
                other_tokens = set(other_sig.lower().replace('_', ' ').split())
                if sig_tokens and other_tokens:
                    jaccard = len(sig_tokens & other_tokens) / len(sig_tokens | other_tokens)
                    if jaccard > 0.3:
                        similar_count += 1
        
        return min(1.0, similar_count / 10.0)
    
    def _compute_entropy(self, pattern: str, all_events: List[Dict]) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾ç¢ºç‡
        total = len(all_events)
        if total == 0:
            return 0.0
        
        pattern_count = sum(1 for e in all_events 
                          if f"{e.get('entity_id')}:{e.get('rule_id')}" == pattern)
        
        p = pattern_count / total
        if p == 0 or p == 1:
            return 0.0
        
        # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        entropy = -p * np.log2(p) - (1-p) * np.log2(1-p)
        return entropy
    
    def train_ensemble(self, events: List[Dict], target_reduction: float = 0.5) -> Dict:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’"""
        features = self.extract_robust_features(events)
        n_events = len(events)
        
        # è¤‡æ•°ã®æˆ¦ç•¥ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        strategies = [
            {'name': 'frequency_based', 'weight_idx': 0, 'threshold': 0.3},
            {'name': 'burst_based', 'weight_idx': 1, 'threshold': 0.4},
            {'name': 'balanced', 'weight_idx': None, 'threshold': 0.25},
        ]
        
        ensemble_predictions = []
        
        for strategy in strategies:
            if strategy['weight_idx'] is not None:
                # ç‰¹å®šã®ç‰¹å¾´ã«é‡ã¿ã‚’ç½®ã
                scores = features[:, strategy['weight_idx']]
            else:
                # ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå…¨ç‰¹å¾´ã®åŠ é‡å¹³å‡ï¼‰
                weights = np.array([0.4, 0.3, 0.2, 0.1, 0.0])
                scores = np.dot(features, weights)
            
            # ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦æŠ‘åˆ¶ã‚’æ±ºå®š
            threshold = strategy['threshold']
            predictions = scores > threshold
            ensemble_predictions.append(predictions)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ï¼ˆå¤šæ•°æ±ºï¼‰
        ensemble_matrix = np.array(ensemble_predictions)
        final_predictions = np.sum(ensemble_matrix, axis=0) >= 2  # 3ã¤ä¸­2ã¤ä»¥ä¸ŠãŒæŠ‘åˆ¶
        
        # ç›®æ¨™å‰Šæ¸›ç‡ã«è¿‘ã¥ã‘ã‚‹ãŸã‚ã®èª¿æ•´
        suppression_count = np.sum(final_predictions)
        target_count = int(n_events * target_reduction)
        
        if suppression_count < target_count:
            # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«è¿½åŠ æŠ‘åˆ¶
            avg_scores = np.mean(features[:, :3], axis=1)  # ä¸»è¦3ç‰¹å¾´ã®å¹³å‡
            sorted_indices = np.argsort(avg_scores)[::-1]
            
            for idx in sorted_indices:
                if not final_predictions[idx]:
                    final_predictions[idx] = True
                    suppression_count += 1
                    if suppression_count >= target_count:
                        break
        
        # çµæœã®é›†è¨ˆ
        suppressed_indices = np.where(final_predictions)[0]
        passed_indices = np.where(~final_predictions)[0]
        
        return {
            'suppressed_indices': suppressed_indices.tolist(),
            'passed_indices': passed_indices.tolist(),
            'suppression_rate': len(suppressed_indices) / n_events if n_events > 0 else 0,
            'features': features.tolist(),
            'ensemble_agreement': np.mean([np.mean(ensemble_matrix[:, i]) for i in range(n_events)])
        }
    
    def cross_validate(self, events: List[Dict], n_folds: int = 3) -> Dict:
        """k-foldäº¤å·®æ¤œè¨¼"""
        n_events = len(events)
        if n_events < n_folds * 2:
            return {'error': 'Not enough data for cross-validation'}
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ±ºå®šçš„ï¼‰
        indices = list(range(n_events))
        np.random.seed(42)  # å›ºå®šã‚·ãƒ¼ãƒ‰
        np.random.shuffle(indices)
        
        fold_size = n_events // n_folds
        cv_results = []
        
        for fold in range(n_folds):
            # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            test_start = fold * fold_size
            test_end = test_start + fold_size if fold < n_folds - 1 else n_events
            test_indices = indices[test_start:test_end]
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            train_indices = indices[:test_start] + indices[test_end:]
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
            train_events = [events[i] for i in train_indices]
            train_result = self.train_ensemble(train_events, target_reduction=0.5)
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
            test_events = [events[i] for i in test_indices]
            test_features = self.extract_robust_features(test_events)
            
            # ç°¡æ˜“çš„ãªè©•ä¾¡ï¼ˆå®Ÿéš›ã®æŠ‘åˆ¶ç‡ï¼‰
            test_suppression_rate = train_result['suppression_rate']
            
            cv_results.append({
                'fold': fold + 1,
                'train_size': len(train_events),
                'test_size': len(test_events),
                'suppression_rate': test_suppression_rate,
                'ensemble_agreement': train_result['ensemble_agreement']
            })
        
        # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        rates = [r['suppression_rate'] for r in cv_results]
        
        return {
            'n_folds': n_folds,
            'mean_suppression_rate': np.mean(rates),
            'std_suppression_rate': np.std(rates),
            'cv_results': cv_results,
            'generalization_score': 1.0 - np.std(rates)  # ä½ã„åˆ†æ•£ = é«˜ã„æ±åŒ–æ€§èƒ½
        }
    
    def generate_production_rules(self, events: List[Dict]) -> Dict:
        """æœ¬ç•ªç’°å¢ƒç”¨ã®ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        # å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
        result = self.train_ensemble(events, target_reduction=0.5)
        features = np.array(result['features'])
        
        # æŠ‘åˆ¶ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        suppressed_indices = result['suppressed_indices']
        suppressed_events = [events[i] for i in suppressed_indices]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦ã‚’é›†è¨ˆ
        pattern_counter = Counter()
        entity_rules = defaultdict(set)
        
        for event in suppressed_events:
            entity = event.get('entity_id', '')
            rule = event.get('rule_id', '')
            pattern = f"{entity}:{rule}"
            pattern_counter[pattern] += 1
            entity_rules[entity].add(rule)
        
        # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
        rules = {
            'version': '2.0',
            'timestamp': datetime.now().isoformat(),
            'suppression_rules': [],
            'threshold_rules': [],
            'ensemble_rules': []
        }
        
        # 1. é«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«
        for pattern, count in pattern_counter.most_common(10):
            if count >= self.config.min_samples:
                parts = pattern.split(':')
                rules['suppression_rules'].append({
                    'pattern': pattern,
                    'entity': parts[0] if parts else '*',
                    'rule': parts[1] if len(parts) > 1 else '*',
                    'min_count': max(2, count // 3),
                    'window': self.config.time_window,
                    'confidence': min(0.9, count / len(suppressed_events))
                })
        
        # 2. é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ãƒ«ãƒ¼ãƒ«
        avg_features = np.mean(features[suppressed_indices], axis=0) if suppressed_indices else np.zeros(5)
        rules['threshold_rules'].append({
            'frequency_threshold': float(avg_features[0]) if len(avg_features) > 0 else 0.5,
            'burst_threshold': float(avg_features[1]) if len(avg_features) > 1 else 0.5,
            'similarity_threshold': float(avg_features[2]) if len(avg_features) > 2 else 0.5,
            'action': 'suppress_if_all_exceed'
        })
        
        # 3. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒ«ãƒ¼ãƒ«
        for entity, rule_set in entity_rules.items():
            if len(rule_set) >= 3:
                rules['ensemble_rules'].append({
                    'entity': entity,
                    'rules': list(rule_set)[:10],
                    'threshold': len(rule_set) // 2,
                    'window': self.config.time_window,
                    'action': 'group_throttle'
                })
        
        return rules


def main():
    parser = argparse.ArgumentParser(description='Robust A/B Optimizer')
    parser.add_argument('--input', required=True, help='Input events file')
    parser.add_argument('--output-rules', default='config/production_rules.json', 
                       help='Output production rules')
    parser.add_argument('--cv-folds', type=int, default=3, 
                       help='Number of cross-validation folds')
    parser.add_argument('--target-reduction', type=float, default=0.5,
                       help='Target reduction rate')
    
    args = parser.parse_args()
    
    # ã‚¤ãƒ™ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
    events = []
    with open(args.input, 'r') as f:
        for line in f:
            if line.strip():
                events.append(json.loads(line))
    
    print(f"ğŸ“¥ {len(events)}ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
    optimizer = RobustOptimizer()
    
    # äº¤å·®æ¤œè¨¼
    print(f"\nğŸ”„ {args.cv_folds}-foldäº¤å·®æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
    cv_results = optimizer.cross_validate(events, n_folds=args.cv_folds)
    
    if 'error' not in cv_results:
        print("\n" + "="*60)
        print("ğŸ“Š äº¤å·®æ¤œè¨¼çµæœ")
        print("="*60)
        print(f"å¹³å‡å‰Šæ¸›ç‡: {cv_results['mean_suppression_rate']*100:.1f}%")
        print(f"æ¨™æº–åå·®: {cv_results['std_suppression_rate']*100:.1f}%")
        print(f"æ±åŒ–ã‚¹ã‚³ã‚¢: {cv_results['generalization_score']:.3f}")
        
        for fold_result in cv_results['cv_results']:
            print(f"\nFold {fold_result['fold']}:")
            print(f"  å‰Šæ¸›ç‡: {fold_result['suppression_rate']*100:.1f}%")
            print(f"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆæ„åº¦: {fold_result['ensemble_agreement']:.3f}")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¦æœ¬ç•ªãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
    print("\nğŸ”§ æœ¬ç•ªç’°å¢ƒç”¨ãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆä¸­...")
    production_rules = optimizer.generate_production_rules(events)
    
    # ãƒ«ãƒ¼ãƒ«ã‚’ä¿å­˜
    with open(args.output_rules, 'w') as f:
        json.dump(production_rules, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ã‚’ä¿å­˜: {args.output_rules}")
    print(f"  æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«: {len(production_rules['suppression_rules'])}å€‹")
    print(f"  é–¾å€¤ãƒ«ãƒ¼ãƒ«: {len(production_rules['threshold_rules'])}å€‹")
    print(f"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ«ãƒ¼ãƒ«: {len(production_rules['ensemble_rules'])}å€‹")
    
    # æœ€çµ‚çš„ãªå­¦ç¿’çµæœ
    final_result = optimizer.train_ensemble(events, target_reduction=args.target_reduction)
    print(f"\nğŸ“ˆ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½:")
    print(f"  å‰Šæ¸›ç‡: {final_result['suppression_rate']*100:.1f}%")
    print(f"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆæ„åº¦: {final_result['ensemble_agreement']:.3f}")
    
    return 0


if __name__ == '__main__':
    exit(main())
